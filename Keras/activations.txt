from keras.layers import Activation, Dense

model.add(Dense(64))
model.add(Activation('tanh'))
等价于：

model.add(Dense(64, activation='tanh'))
你也可以通过传递一个逐元素运算的 Theano/TensorFlow/CNTK 函数来作为激活函数：

from keras import backend as K

model.add(Dense(64, activation=K.tanh))
model.add(Activation(K.tanh))



1.softmax
keras.activations.softmax(x, axis=-1)
Softmax 激活函数。

参数

x：张量。
axis：整数，代表softmax所作用的维度

返回

softmax 变换后的张量。

异常

ValueError：如果 dim(x) == 1


2.elu
keras.activations.elu(x, alpha=1.0)
指数线性单元。

参数

x：张量。
alpha：一个标量，表示负数部分的斜率。
返回

线性指数激活：如果 x > 0，返回值为 x；如果 x < 0 返回值为 alpha * (exp(x)-1)


3.selu
keras.activations.selu(x)
可伸缩的指数线性单元（SELU）。

SELU 等同于：scale * elu(x, alpha)，其中 alpha 和 scale 是预定义的常量。只要正确初始化权重（参见 lecun_normal 初始化方法）并且输入的数量「足够大」（参见参考文献获得更多信息），选择合适的 alpha 和 scale 的值，就可以在两个连续层之间保留输入的均值和方差。

参数

x: 一个用来用于计算激活函数的张量或变量。
返回

可伸缩的指数线性激活：scale * elu(x, alpha)。

注意

与「lecun_normal」初始化方法一起使用。
与 dropout 的变种「AlphaDropout」一起使用。



4.softplus
keras.activations.softplus(x)
Softplus 激活函数。

参数

x: 张量。
返回

Softplus 激活：log(exp(x) + 1)



5.softsign
keras.activations.softsign(x)
Softsign 激活函数。

参数

x: 张量。
返回

Softsign 激活：x / (abs(x) + 1)


5.softsign
keras.activations.softsign(x)
Softsign 激活函数。

参数

x: 张量。
返回

Softsign 激活：x / (abs(x) + 1)



6.relu
keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)
整流线性单元。

使用默认值时，它返回逐元素的 max(x, 0)。

否则，它遵循：

如果 x >= max_value：f(x) = max_value，
如果 threshold <= x < max_value：f(x) = x，
否则：f(x) = alpha * (x - threshold)。
参数

x: 张量。
alpha：负数部分的斜率。默认为 0。
max_value：输出的最大值。
threshold: 浮点数。Thresholded activation 的阈值值。
返回

一个张量。


7.tanh
keras.activations.tanh(x)
双曲正切激活函数。

8.sigmoid
sigmoid(x)
Sigmoid 激活函数。


9.hard_sigmoid
hard_sigmoid(x)
Hard sigmoid 激活函数。

计算速度比 sigmoid 激活函数更快。

参数

x: 张量。
返回

Hard sigmoid 激活：

如果 x < -2.5，返回 0。
如果 x > 2.5，返回 1。
如果 -2.5 <= x <= 2.5，返回 0.2 * x + 0.5。


10.exponential
keras.activations.exponential(x)
自然数指数激活函数。




11.linear
keras.activations.linear(x)
线性激活函数（即不做任何改变）